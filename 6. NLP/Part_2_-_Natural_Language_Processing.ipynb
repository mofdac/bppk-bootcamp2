{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing : Part 2\n",
    "## Feature Generation\n",
    "\n",
    "oleh: \n",
    "<br> Ade Satya Wahana\n",
    "<br> I Gede Yudi Paramartha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Komputer tidak dapat memproses arti dan struktur yang ada dalam suatu teks karena bentuk masih berupa non-numberic. Untuk itu, perlu memproses arti dan struktur dari teks tersebut agar dapat menjadi bentuk angka atau kumpulan angka. \n",
    "\n",
    "Hasil proses mengkonversi teks menjadi angka tersebut akan menjadi feature-feature yang dapat digunakan untuk analisis seperti prediksi sentimen dan lain-lain.\n",
    "\n",
    "Ada banyak teknik feature generation dari teks, namun akan di bahas secara detail salah satu nya yaitu VSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM (Vector Space Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VSM merepresentasikan tiap dokumen (kalimat) dalam sebuah corpus (kumpulan dokumen) sebagai vektor (list/array) di mana setiap element pada vektor tersebut merupakan kemunculan (occurrence) tiap kata/token di dalam dokumen, dimana nilai kemunculan dari tiap kata tersebut dapat pula diberikan pembobotan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ V_{(c)} = \\begin{bmatrix}\n",
    " w_{1,1}&w_{2,1}&...&w_{T,1} \\\\ \n",
    " w_{1,2}&w_{2,2}&...&w_{T,2} \\\\ \n",
    " \\vdots&\\vdots&&\\vdots \\\\\n",
    " w_{1,D}&w_{2,D}&...&w_{T,D}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "- c = corpus\n",
    "- D = jumlah dokumen/kalimat dalam corpus\n",
    "- T = jumlah vocabulary/term/token yang ada dalam seluruh corpus\n",
    "- $w_{1,1}$ = bobot kata pertama pada teks pertama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertimbangan menggunakan VSM:\n",
    "- VSM memiliki asumsi bahwa urutan token di dalam teks tidak terlalu berarti, namun kemunculan tokens lah yang menunjukan arti dan pengaruh token tersebut.\n",
    "- Asumsi tersebut tidak jadi masalah untuk beberapa jenis analisis teks seperti document classification atau clustering. Kumpulan angka dari tiap kata biasanya cukup untuk membedakan semantic concept dari sebuah dokumen.\n",
    "- Ada beberapa jenis analisis teks yang memerlukan informasi berupa urutan kata dalam kalimat seperti information extraction, POS Tagging dan lain-lain\n",
    "- VSM akan menghasilkan matrik yang memiliki dimensi sepanjang jumlah kata dalam corpus. Butuh resources untuk menganalisis matrik tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cara pembobotan yang umum dilakukan dalam VSM:\n",
    "- binary values\n",
    "- bag of word (frequency count)\n",
    "- TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat0= \"Data analisis itu penting\"\n",
    "kalimat1= \"Data cleaning itu penting, sama penting dengan data analisis\"\n",
    "kalimat2= \"Data science melingkupi data analisis dan data cleaning\"\n",
    "\n",
    "gabung = (kalimat0+\" \"+kalimat1+\" \"+kalimat2).lower()\n",
    "vokab = set(gabung.split())\n",
    "vokab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = {'analisis': [1,1,1],\n",
    "          'cleaning':[0,1,1],\n",
    "          'dan': [0,0,1],\n",
    "          'data':[1,1,1],\n",
    "          'dengan':[0,1,0],\n",
    "          'itu':[1,1,0],\n",
    "          'melingkupi':[0,0,1],\n",
    "          'penting':[1,1,1], ## isi ya\n",
    "          'sama':[1,1,1],\n",
    "          'science':[1,1,1]\n",
    "         }\n",
    "pd.DataFrame(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {'analisis': [1,1,1],\n",
    "          'cleaning':[0,1,1],\n",
    "          'dan': [0,0,1],\n",
    "          'data':[1,2,3],\n",
    "          'dengan':[0,1,0],\n",
    "          'itu':[1,1,0],\n",
    "          'melingkupi':[0,0,1],\n",
    "          'penting':[1,1,1],# isi ya\n",
    "          'sama':[1,1,1],\n",
    "          'science':[1,1,1]\n",
    "         }\n",
    "pd.DataFrame(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF = jumlah kemunculan atas satu kata dalam satu dokumen\n",
    "\n",
    "Kenapa digunakan untuk merepresentasikan sebuah dokumen?\n",
    "- kata-kata yang sering muncul (selain stopword) itu memiliki tema (thematic)\n",
    "- jika sebuah kata sering muncul pada sebuah dokumen bertema A, dokumen lain yang mengandung kata tersebut seharusnya memiliki tema A mirip dengan dokumen sebelumnya\n",
    "\n",
    "Cara-cara menghitung TF:\n",
    "- binary: 0/1\n",
    "- raw frequency: $n_{t,d}$\n",
    "- weighted frequency: $\\frac{n_{t,d}}{N_d}$\n",
    "- sublinear TF: $1 + log(n_{t,d}), 0$ if $n_{t,d} = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [kalimat0, kalimat1, kalimat2]\n",
    "\n",
    "#import fungsi countvectorizer dari sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# buat vectorizernya sebagai object, set binary = True juga ingin apply cara hitung binary\n",
    "countVec = CountVectorizer(binary=False)\n",
    "\n",
    "# train vectorizernya dengan corpus (pandas series/list/numpy array)\n",
    "countVec.fit(corpus)\n",
    "\n",
    "# apply vectorizernya ke corpus (data yang sama) dan simpan hasilnya sebagai object\n",
    "hasil = countVec.transform(corpus)\n",
    "\n",
    "# convert hasil menjadi pandas dataframe\n",
    "# hasil harus di convert menjadi numpy array karena sebelumnya memiliki type sparse matrix\n",
    "# ubah kolom dengan feature name yang ada pada vectorizer object\n",
    "pd.DataFrame(hasil.toarray(), columns = countVec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inversed Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masalah jika hanya menggunakan TF:\n",
    "- semua kata/token dianggap sama-sama penting\n",
    "- beberapa kata kadang tidak memiliki atau hanya sedikit kemampuan untuk membedakan antar dokumen.\n",
    "\n",
    "Misalnya, pada kumpulan tweet dari tenaga medis untuk kasus sentiment analisis terkait vaksinasi, kata \"pasien\" atau \"penyakit\" mungkin sering muncul di banyak tweet tapi hanya sedikit berkontribusi untuk  membedakan mana tweet yang bersentimen negatif atau positif.\n",
    "\n",
    "Untuk mengurangi masalah ini, perlu adanya mekanisme untuk mengurangi bobot kata/token yang terlalu sering muncul dalam suatu dokumen agar menjadi lebih bermakna bagi penentuan relevansi. Idenya adalah untuk mengurangi bobot TF tiap kata/token dengan seiring dengan banyaknya dokumen yang memiliki kata tersebut. Faktor pengurang tersebut dihitung dengan menggunakan rumus IDF sebagai berikut:\n",
    "\n",
    "$$idf_t = log \\left ( \\frac{D}{n_t}\\right) $$\n",
    "\n",
    "- D = jumlah dokumen dalam corpus\n",
    "- n_t = jumlah dokumen dimana terdapat term t\n",
    "\n",
    "Cara-cara menghitung IDF:\n",
    "- IDF = $log \\left ( \\frac{D}{n_t}\\right)$\n",
    "- smoothed IDF = $log \\left ( \\frac{D}{n_t+1}\\right)$\n",
    "- Sklearn IDF = $log \\left ( \\frac{D+1}{n_t+1}\\right)+1$\n",
    "- Skleearn IDF, smooth_idf off: $log \\left ( \\frac{D}{n_t}\\right)+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "gabungan antara TF dan IDF dari sebuah term/kata\n",
    "\n",
    "$$ TF-IDF = n_{t,d} \\times log \\left ( \\frac{D}{n_t}\\right) $$\n",
    "\n",
    "semakin banyak kata itu muncul di sebuat dokumen/kalimat, semakin relevan kata tersebut untuk membedakan antar dokumen.\n",
    "namun, semakin banyak kata itu juga muncul di banyak dokumen lain, semakin tidak relevan kata tersebut \n",
    "\n",
    "jika suatu kata di temukan dalam seluruh dokumen makan besar tf-idf nya adalah 0, karena idf nya 0($log\\left ( \\frac{D}{D}\\right )$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fungsi tfidfvectorizer dari sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# buat vectorizernya sebagai object,\n",
    "tfidfVec = TfidfVectorizer()\n",
    "\n",
    "# train vectorizernya dengan corpus (pandas series/list/numpy array)\n",
    "tfidfVec.fit(corpus)\n",
    "\n",
    "# apply vectorizernya ke corpus (data yang sama) dan simpan hasilnya sebagai object\n",
    "hasil2 = tfidfVec.transform(corpus)\n",
    "\n",
    "# convert hasil menjadi pandas dataframe\n",
    "# hasil harus di convert menjadi numpy array karena sebelumnya memiliki type sparse matrix\n",
    "# ubah kolom dengan feature name yang ada pada vectorizer object\n",
    "pd.DataFrame(hasil2.toarray(), columns = tfidfVec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika menggunakan function dari sklearn, cara perhitungannya agak berbeda dari penjelasan di atas. Namun, prinsip inti TF-IDF akan tetap sama: **TF-IDF memberikan nilai yang lebih besar untuk kata-kata yang lebih jarang (muncul di sedikit dokumen) dan tinggi ketika nilai IDF dan TF tinggi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizer lainnya\n",
    "Selain vectorizer-vectorizer yang sudah dijelaskan, terdapat beberapa vectorizer lain yang mungkin dapat dipelajari di luar pelatihan ini. Contoh-contoh nya antara lain:\n",
    "- hashing vectorizer (sklearn menyediakan fungsinya)\n",
    "- word2vec (based on Neural Network, biasanya ada yang menyediakan model nya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tugas\n",
    "\n",
    "gunakan dataset tweet opini film yang sudah dipreprocesing sebelumnya untuk men-generate feature dari text hasil preprocessing. Di bebaskan memilih teknik vectorizer yang mana (binary, bag of words, tf-idf).\n",
    "\n",
    "setelah featurenya digenerate, buat model supervised learning untuk memprediksi sentiment (positif/negatif). Model yang dipergunakan juga dibebaskan, boleh tree based, linear model, KNN, SVM dll. Tapi, tolong gunakan model yang sesuai dengan case ini (*supervised classification case*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
